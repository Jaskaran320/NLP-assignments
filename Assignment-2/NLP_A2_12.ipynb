{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGFBhw_ByE3Y"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "o0Iz9Yft-YK5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1PcKYeSxnUR",
        "outputId": "805441e6-4320-404b-b76e-0ee6d2dbdca3"
      },
      "outputs": [],
      "source": [
        "# request_NER_TRAIN = requests.get(\"http://AdityaAhuja01.pythonanywhere.com/data/NLP_Data/NER_TRAIN_JUDGEMENT.json\")\n",
        "# request_NER_TEST = requests.get(\"http://AdityaAhuja01.pythonanywhere.com/data/NLP_Data/NER_TEST_JUDGEMENT.json\")\n",
        "# request_LR_VAL = requests.get(\"http://AdityaAhuja01.pythonanywhere.com/data/NLP_Data/Laptop_Review_Val.json\")\n",
        "# request_LR_TEST = requests.get(\"http://AdityaAhuja01.pythonanywhere.com/data/NLP_Data/Laptop_Review_Test.json\")\n",
        "# request_LR_TRAIN = requests.get(\"http://AdityaAhuja01.pythonanywhere.com/data/NLP_Data/Laptop_Review_Train.json\")\n",
        "\n",
        "# if not os.path.exists(\"./data\"):\n",
        "#     os.makedirs(\"./data\")\n",
        "\n",
        "# with open(\"./data/NER_train.json\", \"x\") as file:\n",
        "#     file.write(request_NER_TRAIN.text)\n",
        "\n",
        "# with open(\"./data/NER_test.json\", \"x\") as file:\n",
        "#     file.write(request_NER_TEST.text)\n",
        "    \n",
        "# with open(\"./data/LR_Val.json\", \"x\") as file:\n",
        "#     file.write(request_LR_VAL.text)\n",
        "    \n",
        "# with open(\"./data/LR_test.json\", \"x\") as file:\n",
        "#     file.write(request_LR_TEST.text)\n",
        "\n",
        "# with open(\"./data/LR_Train.json\", \"x\") as file:\n",
        "#     file.write(request_LR_TRAIN.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "xSSJ-Dan-eX0"
      },
      "outputs": [],
      "source": [
        "NER_train_file = open(\"./data/NER_train.json\")\n",
        "NER_train_json = json.load(NER_train_file)\n",
        "\n",
        "NER_test_file = open(\"./data/NER_test.json\")\n",
        "NER_test_json = json.load(NER_test_file)\n",
        "\n",
        "LR_train_file = open(\"./data/LR_Train.json\")\n",
        "LR_train_json = json.load(LR_train_file)\n",
        "\n",
        "LR_val_file = open(\"./data/LR_Train.json\")\n",
        "LR_val_json = json.load(LR_val_file)\n",
        "\n",
        "LR_test_file = open(\"./data/LR_Train.json\")\n",
        "LR_test_json = json.load(LR_test_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LR_Preprocessor:\n",
        "    def __init__(self,trainset,testset,valset):\n",
        "        self.trainset = trainset\n",
        "        self.testset = testset\n",
        "        self.valset = valset\n",
        "\n",
        "    def init_tags(self,dataset):\n",
        "        labeled_outputs = []\n",
        "        id = 0\n",
        "        for entry in dataset:\n",
        "            tag_entry = {}\n",
        "            tag_entry[\"id\"] = id \n",
        "            tag_entry[\"text\"] = entry[\"raw_words\"]\n",
        "            tag_entry[\"labels\"] = len(entry[\"words\"])*[\"O\"]\n",
        "            for aspect in entry[\"aspects\"]:\n",
        "                for index in range(aspect[\"from\"],aspect[\"to\"]):\n",
        "                    if index == aspect[\"from\"]:\n",
        "                        tag_entry[\"labels\"][index] = \"B\"\n",
        "                    else:\n",
        "                        tag_entry[\"labels\"][index] = \"I\"\n",
        "                        \n",
        "            labeled_outputs.append(tag_entry)   \n",
        "            id += 1\n",
        "        return labeled_outputs\n",
        "        \n",
        "    def initizalize(self):\n",
        "        self.labeled_trainset = self.init_tags(self.trainset)\n",
        "        self.labeled_valset = self.init_tags(self.valset)\n",
        "        self.labeled_testset = self.init_tags(self.testset)\n",
        "    \n",
        "    def get_tagged_data(self):\n",
        "        return self.labeled_trainset,self.labeled_valset,self.labeled_testset\n",
        "        \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "jkjnXksYHYZN"
      },
      "outputs": [],
      "source": [
        "class NER_Preprocessor:\n",
        "    def __init__(self, trainset, testset, valset=None):\n",
        "        self.trainset = trainset\n",
        "        self.testset = testset\n",
        "        self.valset = valset\n",
        "        if valset is None:\n",
        "            self.split_val()\n",
        "\n",
        "    def split_val(self, split_train_false=False):\n",
        "        if split_train_false:\n",
        "            self.trainset = self.trainset + self.valset\n",
        "        self.trainset, self.valset = train_test_split(\n",
        "            self.trainset, test_size=0.15, random_state=42)\n",
        "\n",
        "    def init_tags(self, dataset):\n",
        "        labeled_output = []\n",
        "        for entry in dataset:\n",
        "            tag_entry = {}\n",
        "            tag_entry[\"id\"] = entry[\"id\"]\n",
        "            annotations = entry[\"annotations\"][0]\n",
        "            sentence = entry[\"data\"][\"text\"]\n",
        "            sentence = sentence.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "            tag_entry[\"text\"] = \"\"\n",
        "            tag_entry[\"labels\"] = []\n",
        "\n",
        "            # empty_tag = False\n",
        "            for word in sentence.split():\n",
        "                not_found = True\n",
        "                for word_range in re.finditer(re.escape(word), sentence):\n",
        "                    skip_check = False\n",
        "                    # if len(annotations[\"result\"]) == 0:\n",
        "                    #     empty_tag = True\n",
        "                    #     break\n",
        "                    for resultobj in annotations[\"result\"]:\n",
        "                        starting_index = resultobj[\"value\"][\"start\"]\n",
        "                        ending_index = resultobj[\"value\"][\"end\"]\n",
        "                        if word_range.start() == starting_index:\n",
        "                            tag_entry[\"text\"] += word + \" \"\n",
        "                            tag_entry[\"labels\"].append(\"B_\" + resultobj[\"value\"][\"labels\"][0])\n",
        "                            not_found = False\n",
        "                            skip_check = True\n",
        "                            break\n",
        "                        elif word_range.start() > starting_index and word_range.end() < ending_index:\n",
        "                            tag_entry[\"text\"] += word + \" \"\n",
        "                            tag_entry[\"labels\"].append(\n",
        "                                \"I_\" + resultobj[\"value\"][\"labels\"][0])\n",
        "                            not_found = False\n",
        "                            skip_check = True\n",
        "                            break\n",
        "                    if skip_check:\n",
        "                        break\n",
        "\n",
        "                if not_found:# and not empty_tag:\n",
        "                    tag_entry[\"text\"] += word + \" \"\n",
        "                    tag_entry[\"labels\"].append(\"O\")\n",
        "            \n",
        "            labeled_output.append(tag_entry)\n",
        "\n",
        "        for i in range(len(labeled_output)):\n",
        "            sentence = \"\"\n",
        "            for word in labeled_output[i][\"text\"].split():\n",
        "                word = re.sub(r'[^\\w\\s]', '', word)\n",
        "                sentence += word + \" \"\n",
        "\n",
        "            labeled_output[i][\"text\"] = sentence\n",
        "\n",
        "        return labeled_output\n",
        "\n",
        "    def initialize(self):\n",
        "        self.labeled_trainset = self.init_tags(self.trainset)\n",
        "        self.labeled_valset = self.init_tags(self.valset)\n",
        "        self.labeled_testset = self.init_tags(self.testset)\n",
        "\n",
        "    def get_tagged(self):\n",
        "        return (self.labeled_trainset, self.labeled_valset, self.labeled_testset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "NER_preprocessor = NER_Preprocessor(NER_train_json, NER_test_json)\n",
        "NER_preprocessor.initialize()\n",
        "NER_train,NER_val,NER_test = NER_preprocessor.get_tagged()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "if (not os.path.exists(\"./processed\")):\n",
        "    os.makedirs(\"./processed\")\n",
        "\n",
        "with open(\"./processed/NER_train_tagged.json\", \"w\") as file:\n",
        "    json.dump(NER_train, file)\n",
        "    \n",
        "with open(\"./processed/NER_val_tagged.json\", \"w\") as file:\n",
        "    json.dump(NER_val, file)\n",
        "\n",
        "with open(\"./processed/NER_test_tagged.json\", \"w\") as file:\n",
        "    json.dump(NER_test, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "LR_Preprocessor = LR_Preprocessor(LR_train_json,LR_test_json,LR_val_json)\n",
        "LR_Preprocessor.initizalize()\n",
        "LR_train,LR_val,LR_test = LR_Preprocessor.get_tagged_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./processed/LR_train_tagged.json\", \"w\") as file:\n",
        "    json.dump(LR_train, file)\n",
        "\n",
        "with open(\"./processed/LR_val_tagged.json\", \"w\") as file:\n",
        "    json.dump(LR_val, file)\n",
        "    \n",
        "with open(\"./processed/LR_test_tagged.json\", \"w\") as file:\n",
        "    json.dump(LR_test, file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
